{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f278e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd666d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which GPU this process can see\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81254d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to training/validation imagery and labels\n",
    "# Each sample image is a multi-band .tif tile\n",
    "# Each label is a single-band .tif\n",
    "# Image and label files share the same filename and live in parallel folders\n",
    "train_data_dir = 'your own link'\n",
    "train_label_dir = 'your own link'\n",
    "validation_data_dir = 'your own link'\n",
    "validation_label_dir = 'your own link'\n",
    "\n",
    "# Data loading & preprocessing\n",
    "# Assumes every image in `data_dir` has a corresponding label with the same filename in `label_dir`.\n",
    "def load_and_preprocess_data(data_dir, label_dir):\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for image_file in os.listdir(data_dir):\n",
    "        if image_file.endswith(\".tif\"):\n",
    "            image_path = os.path.join(data_dir, image_file)\n",
    "            label_path = os.path.join(label_dir, image_file) \n",
    "\n",
    "            image_ds = gdal.Open(image_path, gdal.GA_ReadOnly)\n",
    "            label_ds = gdal.Open(label_path, gdal.GA_ReadOnly)\n",
    "\n",
    "            image = image_ds.ReadAsArray()\n",
    "            label = label_ds.ReadAsArray()\n",
    "\n",
    "\n",
    "            image_list.append(image)\n",
    "            label_list.append(label)\n",
    "\n",
    "            image_ds = None\n",
    "            label_ds = None\n",
    "\n",
    "    image_array = np.array(image_list)\n",
    "    label_array = np.array(label_list)\n",
    "\n",
    "    return image_array, label_array\n",
    "\n",
    "\n",
    "# Loading training and validation set data.\n",
    "X_train, y_train = load_and_preprocess_data(train_data_dir, train_label_dir)\n",
    "X_val, y_val = load_and_preprocess_data(validation_data_dir, validation_label_dir)\n",
    "\n",
    "# One-hot encode labels to 6 classes\n",
    "y_train_one_hot = tf.one_hot(y_train, 6)\n",
    "y_val_one_hot = tf.one_hot(y_val, 6)\n",
    "\n",
    "# Reorder image dimensions from (N, C, H, W) to (N, H, W, C)\n",
    "X_train = X_train.transpose(0, 2, 3, 1)\n",
    "X_val = X_val.transpose(0, 2, 3, 1)\n",
    "\n",
    "# Normalize imagery\n",
    "X_train_normalized = X_train.astype('float32') / 12000\n",
    "X_val_normalized = X_val.astype('float32') / 12000\n",
    "\n",
    "# checks: print shapes\n",
    "print(\"Training data shape:\", X_train_normalized.shape)\n",
    "print(\"Training labels shape:\", y_train_one_hot.shape)\n",
    "print(\"Validation data shape:\", X_val_normalized.shape)\n",
    "print(\"Validation labels shape:\", y_val_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "517fd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModelAfterEpoch(Callback):\n",
    "    \n",
    "    # Keras callback to start tracking a validation metric from a given epoch and save the whole model whenever a new best score is observed.\n",
    "    def __init__(self, filepath, start_epoch=15, monitor='val_accuracy', mode='max'):\n",
    "        super(SaveBestModelAfterEpoch, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.start_epoch = start_epoch\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Initialize the \"best\" value depending on optimization direction\n",
    "        self.best = -float('inf') if mode == 'max' else float('inf')\n",
    "        \n",
    "        # Placeholder if you later want to also keep best weights in-memory\n",
    "        self.best_weights = None\n",
    "\n",
    "    # At the end of each epoch, check the monitored metric in `logs`.If epoch` >= `start_epoch`and performance improves, save the model.\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # Only start tracking/saving after the warm-up period.\n",
    "        if epoch >= self.start_epoch:  =\n",
    "            current = logs.get(self.monitor)\n",
    "            if self.mode == 'max' and current > self.best:\n",
    "                self.best = current\n",
    "                self.model.save(self.filepath, overwrite=True) # Save entire model; overwrite existing best.\n",
    "                print(f'\\nSaving new best model to {self.filepath}')\n",
    "            elif self.mode == 'min' and current < self.best:\n",
    "                self.best = current\n",
    "                self.model.save(self.filepath, overwrite=True)\n",
    "                print(f'\\nSaving new best model to {self.filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net encoderâ€“decoder for 2D semantic segmentation\n",
    "input_layer = Input(shape=(128, 128, 12))  \n",
    "\n",
    "# Encoder (downsampling path)\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "# Decoder (upsampling path)\n",
    "up5 = UpSampling2D(size=(2, 2))(pool4)\n",
    "up5 = Conv2D(256, (3, 3), activation='relu', padding='same')(up5)\n",
    "merge5 = concatenate([conv4, up5], axis=-1)\n",
    "conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge5)\n",
    "\n",
    "up6 = UpSampling2D(size=(2, 2))(conv5)\n",
    "up6 = Conv2D(128, (3, 3), activation='relu', padding='same')(up6)\n",
    "merge6 = concatenate([conv3, up6], axis=-1)\n",
    "conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge6)\n",
    "\n",
    "up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "up7 = Conv2D(64, (3, 3), activation='relu', padding='same')(up7)\n",
    "merge7 = concatenate([conv2, up7], axis=-1)\n",
    "conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(merge7)\n",
    "\n",
    "up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "up8 = Conv2D(32, (3, 3), activation='relu', padding='same')(up8)\n",
    "merge8 = concatenate([conv1, up8], axis=-1)\n",
    "output_layer = Conv2D(6, (1, 1), activation='softmax')(merge8) \n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# save the whole model after epoch >=15 whenever monitored metric improves\n",
    "checkpoint_best_after_15 = SaveBestModelAfterEpoch('your own link.h5')\n",
    "\n",
    "# Compile & Train\n",
    "with tf.device('/GPU:2'):\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(\n",
    "        X_train_normalized, \n",
    "        y_train_one_hot, \n",
    "        validation_data=(X_val_normalized, y_val_one_hot), \n",
    "        epochs=30,\n",
    "        callbacks=[checkpoint_best_after_15],\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "# Evaluate on validation set\n",
    "results = model.evaluate(X_val_normalized, y_val_one_hot)\n",
    "print('Validation loss:', results[0])\n",
    "print('Validation accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdab01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('your own link.h5')\n",
    "\n",
    "# Using the model to make predictions on the validation set\n",
    "y_pred = best_model.predict(X_val_normalized)\n",
    "\n",
    "\n",
    "# Convert the model's output into class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=-1)\n",
    "y_true_labels = np.argmax(y_val_one_hot, axis=-1)\n",
    "\n",
    "print(y_pred_labels.shape)\n",
    "print(y_true_labels.shape)\n",
    "\n",
    "# Flatten to 1D vectors so sklearn metrics treat each pixel as an instance\n",
    "y_pred_labels = y_pred_labels.reshape(-1)\n",
    "y_true_labels = y_true_labels.reshape(-1)\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
